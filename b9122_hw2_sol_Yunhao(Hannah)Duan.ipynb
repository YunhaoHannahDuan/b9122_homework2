{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1 Part1**"
      ],
      "metadata": {
        "id": "u1R5hQlg8VW8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xu26am6Y8P0r"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "BASE_URL = 'https://press.un.org'\n",
        "SEED_URL = BASE_URL + '/en'\n",
        "\n",
        "def get_links_from_page(url):\n",
        "    # Fetch the content of the given URL and return the links that might be press releases.\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find all links containing the word \"crisis\"\n",
        "    links = [BASE_URL + link.get('href') for link in soup.find_all('a') if 'crisis' in link.get_text().lower() and link.get('href')]\n",
        "\n",
        "    return links\n",
        "\n",
        "def is_press_release(url):\n",
        "    # Check if the URL points to a press release.\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Check if there's a \"PRESS RELEASE\" anchor tag\n",
        "    press_release_tag = soup.find('a', hreflang='en', href='/en/press-release')\n",
        "\n",
        "    return response.content if press_release_tag else None\n",
        "\n",
        "def save_content_to_file(content, file_name):\n",
        "    # Save the given content to a file.\n",
        "    with open(file_name, 'w', encoding='utf-8') as f:\n",
        "        f.write(content)\n",
        "\n",
        "def main():\n",
        "    press_release_count = 0\n",
        "    page_number = 0\n",
        "\n",
        "    while press_release_count < 10:\n",
        "        current_page_url = f\"{SEED_URL}/press-release?page={page_number}\"\n",
        "        links = get_links_from_page(current_page_url)\n",
        "\n",
        "        for link in links:\n",
        "            if press_release_count >= 10:  # Check if we already have 10 press releases\n",
        "                return\n",
        "\n",
        "            content = is_press_release(link)\n",
        "            if content:\n",
        "                press_release_count += 1\n",
        "                file_name = f\"1_{press_release_count}.txt\"\n",
        "                save_content_to_file(content.decode('utf-8'), file_name)\n",
        "                time.sleep(2)  # Avoid overwhelming the server\n",
        "\n",
        "        page_number += 1  # Move to the next page\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2 Part2**"
      ],
      "metadata": {
        "id": "qndmrSmT8c5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "BASE_URL = 'https://www.europarl.europa.eu/news/en/press-room/page/'\n",
        "\n",
        "def get_links_from_page(page_number):\n",
        "    # Fetch the content of the given URL and return the links that might be press releases.\n",
        "    response = requests.get(BASE_URL + str(page_number))\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find all links containing the word \"crisis\"\n",
        "    links = [link.get('href') for link in soup.find_all('a') if 'crisis' in link.get_text().lower() and link.get('href')]\n",
        "\n",
        "    return links\n",
        "\n",
        "def is_valid_press_release(url):\n",
        "    # Check if the URL points to a press release that covers plenary sessions.\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Check for \"Plenary session\" text\n",
        "    plenary_session_tag = soup.find('span', class_='ep_name', string='Plenary session')\n",
        "\n",
        "    return response.content if plenary_session_tag else None\n",
        "\n",
        "def save_content_to_file(content, file_name):\n",
        "    # Save the given content to a file.\n",
        "    with open(file_name, 'w', encoding='utf-8') as f:\n",
        "        f.write(content)\n",
        "\n",
        "def main():\n",
        "    press_release_count = 0\n",
        "    page_number = 0\n",
        "\n",
        "    while press_release_count < 10:\n",
        "        links = get_links_from_page(page_number)\n",
        "\n",
        "        for link in links:\n",
        "            if press_release_count >= 10:  # Check if we already have 10 press releases\n",
        "                return\n",
        "\n",
        "            content = is_valid_press_release(link)\n",
        "            if content:\n",
        "                press_release_count += 1\n",
        "                file_name = f\"2_{press_release_count}.txt\"\n",
        "                save_content_to_file(content.decode('utf-8'), file_name)\n",
        "                time.sleep(2)  # Avoid overwhelming the server\n",
        "\n",
        "        page_number += 1  # Move to the next page\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "1KgFkK648fJW"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}